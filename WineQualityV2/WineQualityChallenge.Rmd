---
title: "Wine Quality Challenge"
author: "Luca Marchiori"
date: "2024-03-27"
output:
  html_document: default
  pdf_document: default
---
# Wine Quality Challenge
## Instructions
The inputs include objective tests (e.g. PH values) and the output is based on sensory data (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality between 0 (very bad) and 10 (very excellent).

Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the predicted value and the observed quality. 

RMSE = sqrt(mean((y â€“ haty)^2)) 

## Dataset colums
1. fixed acidity
2. volatile acidity
3. citric acid
4. residual sugar
5. chlorides
6. free sulfur dioxide
7. total sulfur dioxide
8. density
9. pH
10. sulphates
11. alcohol
12. quality (score between 0 and 10)

## Init
```{r}
# Load libraries
library(caret)
library(ggplot2)
library(corrplot)
library(ggbiplot)
library(rpart)
library(rpart.plot)
library(randomForest)
#Clear the workspace
rm(list=ls())
```

## Data import
```{r}
# Load the training data
train <- read.csv("wineq_train.csv", stringsAsFactors=F)
# Load the test data
test <- read.csv("wineq_validation.csv", , stringsAsFactors=F)
```
## Helpers
### Minimal Squared Error
MSE (Mean Squared Error) is a common metric for evaluating the performance of regression models, by comparing the observed values `y` with the predicted values `yhat` generated by the provided `model`.
```{r}
MSE <- function(y, model){
  yhat = predict(model) # Estimated y computed on the basis of the features
  mean((y-yhat)^2)
}

RMSE <- function(y, model){
  sqrt(MSE(y,model))
}
```
## Exploratory Data Analysis
```{r}
# Compactly Display the Structure of an Arbitrary R Object
str(train)

# Take an initial view of the dataset
summary(train)
```
All the features are numeric, no qualitative data is present. The quality is the target variable and since it can be considered as a continuous variable, we can use regression models to predict it.

### Scatterplots
With the following scatterplots, we can see the relationship between the quality and the other features. Each level of quality is represented by a different color.
```{r}
# Scatterplot matrix, with different colors for each quality level
pairs(train, col = train$quality, upper.panel = NULL, pch = 19, cex = 0.5)

# Single scatterplots of features against quality
# plot(train$alcohol, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$total.sulfur.dioxide, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$free.sulfur.dioxide, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$volatile.acidity, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$fixed.acidity, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$pH, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$density, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$sulphates, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$chlorides, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$residual.sugar, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$citric.acid, train$quality, col = train$quality, pch = 19, cex = 0.5)

```
### Principal Component Analysis
Here, PCA is used to identify the most important features in the dataset in order to better understand the data. Here it is only used for visualization purposes, as trees and random forests are better suited for feature selection in this case. The biplot shows the relationship between the features and the quality of the wine. 
By default, the prcomp() function, with the option "scale = true", centers and scales the data, so we don't need to do it manually.
```{r}
# Principal Component Analysis
pca <- prcomp(train[,1:12], scale = TRUE)
summary(pca)

# Biplot with GGplot2
ggbiplot::ggbiplot(pca, obs.scale = 1, var.scale = 1, groups = train$quality, ellipse = TRUE, circle = TRUE, varname.size = 3, varname.adjust = 3, varname.color = "orange", varname.face = "bold") +
  theme_minimal() +
  theme(legend.title = element_text(size = 10), legend.text = element_text(size = 10)) +
  ggtitle("PCA Biplot")
```

### Correlation heatmap
The correlation heatmap shows the correlation between the features. The correlation matrix is calculated using the cor() function, and the heatmap is plotted using the corrplot() function. In this dataset, alcohol, density, chlorides, and volatile acidity are the most correlated features with quality.
```{r}
corrplot(cor(train[,1:12]), method = "number", tl.col = "black", tl.cex = 0.7, number.cex = 0.7)
```

## Feature selection
In this sections, forward step wise selection, decision trees and random forest will be used to select the most important features and train the models.

### Forward Stepwise Selection
```{r}
baseModel <- lm(quality ~ 1, data = train)
FwStepSel <- step(baseModel, direction = "forward", scope = list(lower = baseModel, upper = lm(quality ~ ., data = train)))

#Based on the previous step results:
fit = lm(formula = FwStepSel$call$formula, data = train)
```

### Decision trees
```{r}
# Fit a complete decision tree
dTreefit <- rpart(quality ~ ., data = train, method = "anova", cp = 0.005)

# Plot the decision tree
rpart.plot(dTreefit, type = 4, extra = 101, under = TRUE, fallen.leaves = TRUE, cex = 0.5, tweak = 1.2)

# Get the importance of each feature
feature_importance <- data.frame(Feature = names(dTreefit$variable.importance), Importance = dTreefit$variable.importance)

# Plot feature importance
barplot(feature_importance$Importance, names.arg = feature_importance$Feature, las = 2,main = "Feature Importance")

# Fit a decision tree on a fewer features
dTreefit <- rpart(quality ~ . -citric.acid - pH - fixed.acidity - sulphates, data=train)
rpart.plot(dTreefit)
#yhat = predict(dTreefit, newdata=test)
#write.table(file="fitDecisionTreeSubsetFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

rmse = RMSE(y=train$quality, model=dTreefit)
rmse # 0.7498735

```

### Random forests
```{r}
# Train the random forest model with all features
rForestfit <- randomForest(quality ~ ., train)

# Get the importance of each feature and plot it
importance <- importance(rForestfit)
varImpPlot(rForestfit)

# Predict the quality of the test set with all features
yhat = predict(rForestfit, newdata=test)
#write.table(file="fitRandomForestAllFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

rmse = RMSE(y=train$quality, model=rForestfit)
rmse # 0.6022833

# Train the random forest model with selected features
rForestfit <- randomForest(quality ~ . - sulphates - fixed.acidity - citric.acid - pH, train)
yhat = predict(rForestfit, newdata=test)
write.table(file="fitRandomForestSubsetFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

rmse = RMSE(y=train$quality, model=rForestfit)
rmse # 0.6185242
```

### Feature selection and tree models considerations
From the stepwise selection, we get that the best model is the one with the following features:  alcohol + volatile.acidity + residual.sugar + free.sulfur.dioxide + density + pH + sulphates + fixed.acidity.

From the decision trees, we can exclude the features citric.acid, pH, fixed.acidity, and sulphates.

From the random forest, we can exclude the features sulphates, fixed.acidity, citric.acid, and pH.

Even though the random forest model trained with all features has the lowest RMSE, it is better to use the model trained with the selected features to avoid overfitting. In fact, the model trained with the feature subset gives a better result when loaded to the DataChallenge platform.

## Linear model
For the scope of this challenge, a linear model is trained using the features selected with the forward stepwise selection. Cross validation is used to evaluate the model.


