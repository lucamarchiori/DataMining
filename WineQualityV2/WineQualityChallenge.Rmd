---
title: "Wine Quality Challenge"
author: "Luca Marchiori"
date: "2024-03-27"
output:
  html_document: default
  pdf_document: default
---

# Wine Quality Challenge

This report, related to the Data Mining course (A.Y. 2023-24), is about the Wine Quality Challenge. The goal of the challenge is to predict the quality of wine based on its chemical properties. The dataset contains 12 features and the target variable is the quality of the wine, which is a continuous variable.

```{r, include=FALSE}
# Load libraries
library(caret)
library(ggplot2)
library(corrplot)
library(ggbiplot)
library(rpart)
library(rpart.plot)
library(randomForest)
library(leaps)
library(tree)
#Clear the workspace
rm(list=ls())
```

```{r, include=FALSE}
# Load the training data
train <- read.csv("wineq_train.csv", stringsAsFactors=F)
# Load the test data
test <- read.csv("wineq_validation.csv", stringsAsFactors=F)
```

```{r setup, include=FALSE}

#MSE (Mean Squared Error) is a common metric for evaluating the performance of regression models, by comparing the observed values `y` with the predicted values `yhat` generated by the provided `model`.

MSE <- function(y, model){
  yhat = predict(model) # Estimated y computed on the basis of the features
  mean((y-yhat)^2)
}

RMSE <- function(y, model){
  sqrt(MSE(y,model))
}
```

## Exploratory Data Analysis

First of all, after the dataset is loaded, a summary of the dataset is displayed to get an initial view of the data. Correlation plots of the features are displayed to see the relationship between them.

The dataset is then checked for outliers and some of them are removed. For outliers, we consider values that are too far from the other values in the dataset, and we remove them because they are likely to be errors in the data. To identify outliers, both boxplots and scatterplots are displayed. Then a treshold is set for each feature and the observations that exceed the threshold are removed.

```{r}
# Compactly Display the Structure of the dataset
str(train)

# Take an initial view of the dataset
summary(train)
```

All the features are numeric, no qualitative data is present. The quality is the target variable and since it can be considered as a continuous variable, we can use regression models to predict it.

### Outliers detection

```{r, include=FALSE}
# Boxplot of the features
par(mfrow=c(3,4))
for (i in 1:12) {
  boxplot(train[,i], main = colnames(train)[i])
}

```

```{r}
# Plot feature values to find outliers
par(mfrow=c(3,4))
plot(train$residual.sugar, main = "Residual Sugar", pch = 19, cex = 0.5)
plot(train$density, main = "Density", pch = 19, cex = 0.5)
plot(train$chlorides, main = "Chlorides", pch = 19, cex = 0.5)
plot(train$free.sulfur.dioxide, main = "Free Sulfur Dioxide", pch = 19, cex = 0.5)
plot(train$total.sulfur.dioxide, main = "Total Sulfur Dioxide", pch = 19, cex = 0.5)
plot(train$volatile.acidity, main = "Volatile Acidity", pch = 19, cex = 0.5)
plot(train$fixed.acidity, main = "Fixed Acidity", pch = 19, cex = 0.5)
plot(train$pH, main = "pH", pch = 19, cex = 0.5)
plot(train$sulphates, main = "Sulphates", pch = 19, cex = 0.5)
plot(train$alcohol, main = "Alcohol", pch = 19, cex = 0.5)
plot(train$citric.acid, main = "Citric Acid", pch = 19, cex = 0.5)
plot(train$quality, main = "Quality", pch = 19, cex = 0.5)
```

```{r, include=FALSE}
# Remove dataset feature if residual sugar is higher than 40
train <- train[train$residual.sugar < 40,]
# Remove dataset feture if density is higher than 1.005
train <- train[train$density < 1.005,]
# Remove dataset feature if free sulfur dioxide is higher than 200
train <- train[train$free.sulfur.dioxide < 200,]
# Remove dataset feature if total sulfur dioxide is higher than 350
train <- train[train$total.sulfur.dioxide < 350,]
# Remove dataset feature if fixed acidity is higher than 15
train <- train[train$fixed.acidity < 15,]
```

### Scatterplots

With the following scatterplots, we can see the relationship between the quality and the other features. Each level of quality is represented by a different color.

```{r}
# Scatterplot matrix, with different colors for each quality level
pairs(train, col = train$quality, upper.panel = NULL, pch = 19, cex = 0.5)

# Single scatterplots of features against quality
# plot(train$alcohol, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$total.sulfur.dioxide, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$free.sulfur.dioxide, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$volatile.acidity, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$fixed.acidity, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$pH, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$density, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$sulphates, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$chlorides, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$residual.sugar, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$citric.acid, train$quality, col = train$quality, pch = 19, cex = 0.5)

```

### Principal Component Analysis

Here, PCA is used to identify the most important features in the dataset in order to better understand the data. The biplot shows the relationship between the features and the quality of the wine. By default, the prcomp() function, with the option "scale = true", centers and scales the data, so we don't need to do it manually.

```{r}
# Principal Component Analysis
pca <- prcomp(train[,1:12], scale = TRUE)
summary(pca)

# Biplot with GGplot2
ggbiplot::ggbiplot(pca, obs.scale = 1, var.scale = 1, groups = train$quality, ellipse = TRUE, circle = TRUE, varname.size = 3, varname.adjust = 3, varname.color = "orange", varname.face = "bold") +
  theme_minimal() +
  theme(legend.title = element_text(size = 10), legend.text = element_text(size = 10)) +
  ggtitle("PCA Biplot")
```

### Correlation heatmap

The correlation heatmap shows the correlation between the features. The correlation matrix is calculated using the cor() function, and the heatmap is plotted using the corrplot() function. In this dataset, alcohol, density, chlorides, and volatile acidity are the most correlated features with quality.

```{r}
corrplot(cor(train[,1:12]), method = "number", tl.col = "black", tl.cex = 0.7, number.cex = 0.7)
```

## Linear Model

### Model training

The wine quality challenge provides a test set of 1200 observations to evaluate the model. Instead of use it as a validation set, i will use the cross-validation technique to evaluate the model. The choice falls on a LOOCV (Leave-One-Out Cross-Validation) because the dataset is not too large, because we are evaluating a linear model (for wich LOOCV is optimezed), and because it is a good way to avoid overfitting.

First of all, a linear model is trained on all the features of the dataset using the caret package. The model is trained with the LOOCV method and the RMSE is calculated. After that i use the test data to predict the quality of the wine. The predictions are saved in a file for the datachallenge submission and the corresponding RMSE is calculated.

The resulting RMSE, calculated with LOOCV, is 0.7508557. The RMSE calculated on the training data is 0.7482341.

```{r, include=FALSE}
# Set the formula for the linear model on all the features of the dataset
lmFormula <- formula(quality ~ .)
# Set the cross-validation method
loocv <- trainControl(method = "LOOCV")
# Fit the linear model
lmFit <- caret::train(lmFormula, data = train, method = "lm", trControl = loocv, preProcess = c("center", "scale"))
lmFit$results$RMSE #0.7508557 

# Get the summary of the model
summary(lmFit)

# Predict the quality of the test set
yhat = predict(lmFit, newdata=test)
write.table(file="./DatachallengeSubmissions/fitLinearModelAllFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

# Calculate the RMSE of the model
rmse = RMSE(y=train$quality, model=lmFit)
rmse # 0.7482341
```

Now that a linear model with all the features is trained, i will use the stepwise selection to select a subset of most important features for the linear model. The model is trained using with the "glmStepAIC" provided by the caret package and the RMSE is calculated again both with cross validation and the test data. This time, LOOCV is too slow to run because the stepwise selection will train the model multiple times (by adding more and more features). For this reason, i will use the K-fold cross-validation method with k=10.

The features excluded by the stepwise selection are: citric.acid, chlorides, total.sulfur.dioxide. A new model is trained with a subset of features. The resulting RMSE (with cross-validation) is 0.7506327 which is sligthly better than the previous linear model.

```{r}
# Set the cross-validation method as k-fold cross-validation
kCv <- trainControl(method = "cv", number = 10)
# Train with stepwise selection
fwStepLm <- caret::train(quality ~ ., data = train, method = "glmStepAIC", trControl = kCv, preProcess = c("center", "scale"), trace = FALSE)
# Print the result RMSE
fwStepLm$results$RMSE # 0.7506327
# Get the selected features
fwStepLm$finalModel$formula

# Fit the linear model with the selected features and calculate the RMSE of the model with LOOCV to compare it with the model with all features. The features not selected are citric.acid, chlorides, total.sulfur.dioxide.
selectedFormula <- formula(quality ~ fixed.acidity + volatile.acidity + residual.sugar + free.sulfur.dioxide + density + pH + sulphates + alcohol)
loocv <- trainControl(method = "LOOCV")
lmFit <- caret::train(selectedFormula, data = train, method = "lm", trControl = loocv, preProcess = c("center", "scale"))
lmFit$results$RMSE #0.7504533

# Predict the quality of the test set
yhat = predict(lmFit, newdata=test)
write.table(file="./DatachallengeSubmissions/fitLinearModelSubsetFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

# Calculate the RMSE of the model on test data
rmse = RMSE(y=train$quality, model=lmFit)
rmse # 0.7483839

```

## Tree-Based methods

Since the results obtained with linear models are not the best, both decision trees and random forests are now used to predict the quality of the wine.

### Regression trees
First of all, the regression tree is trained on all the features using a very low pruning level. By using cross validation, it is possible to calculate and plot the deviance of the model at different pruning levels. From the plot, it is possible to see that after a pruning level of 6, the deviance is not improved, instead the risk of overfitting increases so the tree is pruned to the optimal level. The predictions are saved in a file for the datachallenge submission and the corresponding RMSE is calculated. Alcohol, volatile acidity, and free sulfur dioxide are the most important features in the tree and are those that impact the quality of the wine the most. With this dataset, the regression tree performs better than the linear model.

```{r, include=FALSE}
# Fit a regression tree on all the features with a very small pruning level
rTreeFit <- tree(quality ~ ., data = train, control = tree.control(nobs = nrow(train), mindev = 0.005))

# Plot the tree
summary(rTreeFit)
plot(rTreeFit)
text(rTreeFit, pretty = 0)

# Cross-validation to find the optimal pruning level using the deviance
rTreeCv <- cv.tree(rTreeFit, method = "deviance")
plot(rTreeCv$size, rTreeCv$dev, type = "b", xlab = "Tree Size", ylab = "Deviance", main = "Pruning VS Deviance Plot")

# Prune the tree to the optimal level
prunedTree <- prune.tree(rTreeFit, best = 6)

# Predict the quality of the test set
yhat = predict(prunedTree, newdata=test)
write.table(file="./DatachallengeSubmissions/fitRegressionTreeAllFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

# Calculate the RMSE of the model
rmse = RMSE(y=train$quality, model=prunedTree)
rmse # 0.7250806

# Plot the final pruned tree
summary(prunedTree)
plot(prunedTree)
text(prunedTree, pretty = 1)

```

### Random forests
As an alternative to regression trees, random forests can be used to predict the quality of the wine. Here, a grid search is performed to find the optimal mtry value for the random forest model. The mtry value is the number of features randomly sampled at each split of the tree. Cross validation is used to evaluate the different models. The random forest model with mtry = 2 is the best model trained so far. A prediction on the test set is made and uploaded to the datachallenge submission.

```{r}
# Define a grid of mtry values to search over
tuneGrid <- expand.grid(mtry = seq(1, ncol(train) - 1, by = 1))

# Set up cross-validation control
rForestCtrl <- trainControl(method = "cv", number = 5, search = "grid")

# Train the model with the tuning grid
rForestFit <- train(quality ~ ., data = train, method = "rf", trControl = rForestCtrl, preProcess = c("center", "scale"), tuneGrid = tuneGrid)

# Get the importance of each feature and plot it
importance <- importance(rForestFit$finalModel)
varImpPlot(rForestFit$finalModel)

# Predict the quality of the test set
yhat = predict(rForestFit, newdata = test)
write.table(file = "./DatachallengeSubmissions/fitRandomForestAllFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

# Calculate the RMSE of the model on the training data
rmse = RMSE(y=train$quality, model=rForestFit$finalModel)
print(rmse) # 0.6065538

# Get the final mtry value
rForestFit$finalModel$mtry

```

## Conclusions
For this challenge, the best model is the random forest model with mtry = 2 (RMSE of 0.6065538), the results uploaded to the Datachallenge platform confirm that, among the models viewed during the course, the random forest is the best model for this dataset. Applying a grid search on mtry and cross validation is very computationally expensive, but it allowed to find a good model. In the case of bigger datasets, it would be better to use a smaller grid search or a random search to find the optimal parameters.