---
title: "Wine Quality Challenge"
author: "Luca Marchiori"
date: "2024-03-27"
output:
  html_document: default
  pdf_document: default
---
# Wine Quality Challenge
## Instructions
The inputs include objective tests (e.g. PH values) and the output is based on sensory data (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality between 0 (very bad) and 10 (very excellent).

Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the predicted value and the observed quality. 

RMSE = sqrt(mean((y â€“ haty)^2)) 

## Dataset colums
1. fixed acidity
2. volatile acidity
3. citric acid
4. residual sugar
5. chlorides
6. free sulfur dioxide
7. total sulfur dioxide
8. density
9. pH
10. sulphates
11. alcohol
12. quality (score between 0 and 10)

## Init
```{r}
# Load libraries
library(caret)
library(ggplot2)
library(corrplot)
library(ggbiplot)
library(rpart)
library(rpart.plot)
library(randomForest)
library(leaps)
library(tree)
#Clear the workspace
rm(list=ls())
```

## Data import
```{r}
# Load the training data
train <- read.csv("wineq_train.csv", stringsAsFactors=F)
# Load the test data
test <- read.csv("wineq_validation.csv", stringsAsFactors=F)
```
## Helpers
### Minimal Squared Error
MSE (Mean Squared Error) is a common metric for evaluating the performance of regression models, by comparing the observed values `y` with the predicted values `yhat` generated by the provided `model`.
```{r}
MSE <- function(y, model){
  yhat = predict(model) # Estimated y computed on the basis of the features
  mean((y-yhat)^2)
}

RMSE <- function(y, model){
  sqrt(MSE(y,model))
}
```
## Exploratory Data Analysis
Initial investigation and analysis of the dataset to understand its key characteristics, identify data quality issues and understand relationships between variables.
```{r}
# Compactly Display the Structure of the dataset
str(train)

# Take an initial view of the dataset
summary(train)
```
All the features are numeric, no qualitative data is present. The quality is the target variable and since it can be considered as a continuous variable, we can use regression models to predict it.

### Outliers detection
```{r}
# Boxplot of the features
par(mfrow=c(3,4))
for (i in 1:12) {
  boxplot(train[,i], main = colnames(train)[i])
}

# Plot feature values to find outliers
plot(train$residual.sugar, main = "Residual Sugar", pch = 19, cex = 0.5)
plot(train$density, main = "Density", pch = 19, cex = 0.5)
plot(train$chlorides, main = "Chlorides", pch = 19, cex = 0.5)
plot(train$free.sulfur.dioxide, main = "Free Sulfur Dioxide", pch = 19, cex = 0.5)
plot(train$total.sulfur.dioxide, main = "Total Sulfur Dioxide", pch = 19, cex = 0.5)
plot(train$volatile.acidity, main = "Volatile Acidity", pch = 19, cex = 0.5)
plot(train$fixed.acidity, main = "Fixed Acidity", pch = 19, cex = 0.5)
plot(train$pH, main = "pH", pch = 19, cex = 0.5)
plot(train$sulphates, main = "Sulphates", pch = 19, cex = 0.5)
plot(train$alcohol, main = "Alcohol", pch = 19, cex = 0.5)
plot(train$citric.acid, main = "Citric Acid", pch = 19, cex = 0.5)
plot(train$quality, main = "Quality", pch = 19, cex = 0.5)
```
Outliers listed below are removed from the dataset because they are likely to be errors in the data since they are very far from the other values.
```{r}
# Remove dataset feature if residual sugar is higher than 40
train <- train[train$residual.sugar < 40,]
# Remove dataset feture if density is higher than 1.005
train <- train[train$density < 1.005,]
# Remove dataset feature if free sulfur dioxide is higher than 200
train <- train[train$free.sulfur.dioxide < 200,]
# Remove dataset feature if total sulfur dioxide is higher than 350
train <- train[train$total.sulfur.dioxide < 350,]
# Remove dataset feature if fixed acidity is higher than 15
train <- train[train$fixed.acidity < 15,]
```


### Scatterplots
With the following scatterplots, we can see the relationship between the quality and the other features. Each level of quality is represented by a different color.
```{r}
# Scatterplot matrix, with different colors for each quality level
pairs(train, col = train$quality, upper.panel = NULL, pch = 19, cex = 0.5)

# Single scatterplots of features against quality
# plot(train$alcohol, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$total.sulfur.dioxide, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$free.sulfur.dioxide, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$volatile.acidity, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$fixed.acidity, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$pH, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$density, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$sulphates, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$chlorides, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$residual.sugar, train$quality, col = train$quality, pch = 19, cex = 0.5)
# plot(train$citric.acid, train$quality, col = train$quality, pch = 19, cex = 0.5)

```
### Principal Component Analysis
Here, PCA is used to identify the most important features in the dataset in order to better understand the data. Here it is only used for visualization purposes, as trees and random forests are better suited for feature selection in this case. The biplot shows the relationship between the features and the quality of the wine. 
By default, the prcomp() function, with the option "scale = true", centers and scales the data, so we don't need to do it manually.
```{r}
# Principal Component Analysis
pca <- prcomp(train[,1:12], scale = TRUE)
summary(pca)

# Biplot with GGplot2
ggbiplot::ggbiplot(pca, obs.scale = 1, var.scale = 1, groups = train$quality, ellipse = TRUE, circle = TRUE, varname.size = 3, varname.adjust = 3, varname.color = "orange", varname.face = "bold") +
  theme_minimal() +
  theme(legend.title = element_text(size = 10), legend.text = element_text(size = 10)) +
  ggtitle("PCA Biplot")
```

### Correlation heatmap
The correlation heatmap shows the correlation between the features. The correlation matrix is calculated using the cor() function, and the heatmap is plotted using the corrplot() function. In this dataset, alcohol, density, chlorides, and volatile acidity are the most correlated features with quality.
```{r}
corrplot(cor(train[,1:12]), method = "number", tl.col = "black", tl.cex = 0.7, number.cex = 0.7)
```

## Linear Model

### Model training

The wine quality challenge provides a test set of 1200 observations to evaluate the model. Instead of use it as a validation set, i will use the cross-validation technique to evaluate the model. The choice falls on a LOOCV (Leave-One-Out Cross-Validation) because the dataset is not too large, because we are evaluating a linear model (for wich LOOCV is optimezed), and because it is a good way to avoid overfitting.

First of all, a linear model is trained on all the features of the dataset using the caret package. The model is trained with the LOOCV method and the RMSE is calculated.

After that i use the test data to predict the quality of the wine. The predictions are saved in a file for the datachallenge submission and the corresponding RMSE is calculated.

```{r}
# Set the formula for the linear model on all the features of the dataset
lmFormula <- formula(quality ~ .)
# Set the cross-validation method
loocv <- trainControl(method = "LOOCV")
# Fit the linear model
lmFit <- caret::train(lmFormula, data = train, method = "lm", trControl = loocv, preProcess = c("center", "scale"))
lmFit$results$RMSE #0.7508557 

# Get the summary of the model
summary(lmFit)

# Predict the quality of the test set
yhat = predict(lmFit, newdata=test)
write.table(file="./DatachallengeSubmissions/fitLinearModelAllFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

# Calculate the RMSE of the model
rmse = RMSE(y=train$quality, model=lmFit)
rmse # 0.7482341
```

Now that a linear model with all the features is trained, i will use the stepwise selection to select a subset of most important features for the linear model.
The model is trained using with the "glmStepAIC" provided by the caret package abd the RMSE is calculated again both with cross validation and the test data.
This time, LOOCV is too slow to run because the stepwise selection will train the model multiple times (by adding more and more features). For this reason, i will use the K-fold cross-validation method with k=10.

```{r}
# Set the cross-validation method as k-fold cross-validation
kCv <- trainControl(method = "cv", number = 10)
# Train with stepwise selection
fwStepLm <- caret::train(quality ~ ., data = train, method = "glmStepAIC", trControl = kCv, preProcess = c("center", "scale"), trace = FALSE)
# Print the result RMSE
fwStepLm$results$RMSE # 0.7506327
# Get the selected features
fwStepLm$finalModel$formula

# Fit the linear model with the selected features and calculate the RMSE of the model with LOOCV to compare it with the model with all features. The features not selected are citric.acid, chlorides, total.sulfur.dioxide.
selectedFormula <- formula(quality ~ fixed.acidity + volatile.acidity + residual.sugar + free.sulfur.dioxide + density + pH + sulphates + alcohol)
loocv <- trainControl(method = "LOOCV")
lmFit <- caret::train(selectedFormula, data = train, method = "lm", trControl = loocv, preProcess = c("center", "scale"))
lmFit$results$RMSE #0.7504533

# Predict the quality of the test set
yhat = predict(lmFit, newdata=test)
write.table(file="./DatachallengeSubmissions/fitLinearModelSubsetFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

# Calculate the RMSE of the model on test data
rmse = RMSE(y=train$quality, model=lmFit)
rmse # 0.7483839

```

The RMSE of the model with a subset of features is slightly lower than the one with all the features.

## Tree-Based methods
In this section, decision trees and random forests are used to predict the quality of the wine.

### Regression trees
First of all, the regression tree is trained on all the features using a very low pruning level. By using cross validation, it is possible to calculate and plot the deviance of the model at different pruning levels. From the plot, it is possible to see that after a pruning level of 6, the deviance is not improved, instead the risk of overfitting increases so the tree is pruned to the optimal level. The predictions are saved in a file for the datachallenge submission and the corresponding RMSE is calculated.

Alcohol, volatile acidity, and free sulfur dioxide are the most important features in the tree and are those that impact the quality of the wine the most.

With this dataset, the regression tree performs better than the linear model.

```{r}
# Fit a regression tree on all the features with a very small pruning level
rTreeFit <- tree(quality ~ ., data = train, control = tree.control(nobs = nrow(train), mindev = 0.005))

# Plot the tree
summary(rTreeFit)
plot(rTreeFit)
text(rTreeFit, pretty = 0)

# Cross-validation to find the optimal pruning level using the deviance
rTreeCv <- cv.tree(rTreeFit, method = "deviance")
plot(rTreeCv$size, rTreeCv$dev, type = "b", xlab = "Tree Size", ylab = "Deviance", main = "Pruning VS Deviance Plot")

# Prune the tree to the optimal level
prunedTree <- prune.tree(rTreeFit, best = 6)

# Predict the quality of the test set
yhat = predict(prunedTree, newdata=test)
write.table(file="./DatachallengeSubmissions/fitRegressionTreeAllFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

# Calculate the RMSE of the model
rmse = RMSE(y=train$quality, model=prunedTree)
rmse # 0.7250806

# Plot the final pruned tree
summary(prunedTree)
plot(prunedTree)
text(prunedTree, pretty = 1)

```

### Random forests
As an alternative to regression trees, random forests can be used to predict the quality of the wine.
Here, a grid search is performed to find the optimal mtry value for the random forest model. The mtry value is the number of features randomly sampled at each split of the tree. Cross validation is used to evaluate the different models.

The random forest model with mtry = 2 is the best model trained so far. A prediction on the test set is made and uploaded to the datachallenge submission.

```{r}
# Define a grid of mtry values to search over
tuneGrid <- expand.grid(mtry = seq(1, ncol(train) - 1, by = 1))

# Set up cross-validation control
rForestCtrl <- trainControl(method = "cv", number = 5, search = "grid")

# Train the model with the tuning grid
rForestFit <- train(quality ~ ., data = train, method = "rf", trControl = rForestCtrl, preProcess = c("center", "scale"), tuneGrid = tuneGrid)

# Get the importance of each feature and plot it
importance <- importance(rForestFit$finalModel)
varImpPlot(rForestFit$finalModel)

# Predict the quality of the test set
yhat = predict(rForestFit, newdata = test)
write.table(file = "./DatachallengeSubmissions/fitRandomForestAllFeatures.txt", yhat, row.names = FALSE, col.names = FALSE)

# Calculate the RMSE of the model on the training data
rmse = RMSE(y=train$quality, model=rForestFit$finalModel)
print(rmse) # 0.6065538

# Get the final mtry value
rForestFit$finalModel$mtry

```







