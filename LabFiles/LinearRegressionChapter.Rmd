---
title: "LinearRegressionChapter"
author: "Luca Marchiori"
date: "2024-04-14"
output: html_document
---

```{r}
# Get the advertising data from https://www.statlearning.com/s/Advertising.csv
dati <- read.csv("https://www.statlearning.com/s/Advertising.csv")
dati$X <- NULL
head(dati)
summary(dati)

```

# Simple Linear Regression
This is a very straightforward simple linear approach for predicting a quantitative response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y. Mathematically, we can write this linear relationship as: $Y ≈ \beta_0 + \beta_1X$.

In relation to the Sales data, we can write the linear relationship as: $Sales ≈ \beta_0 + \beta_1TV$.

$\beta_0$ and $\beta_1$ are two unknown constants that represent the intercept and slope terms in the linear model. Together are known as the model coefficients or parameters. Once we have used our slope training data to produce estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ for the model coefficients, we can predict future sales on the basis of a particular value of TV advertising parameter by computing $\hat{Y} = \hat{\beta_0} + \hat{\beta_1}X$. We use a hat symbol to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response.


```{r}
# Fit a simple linear regression model to predict Sales using TV advertising
lm.fit <- lm(sales ~ TV, data = dati)
```

## Estimating the Coefficients
In practice, $\beta_0$ and $\beta_1$ are unknown. So before we can make predictions, we must use data to estimate the coefficients. We want to find an intercept $\beta_0$ and a slope $\beta_1$ such that the resulting line is as close as possible to the data points. To do this, the most common approach involves minimizing the least squares criterion.

Let $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}X_i$ be the prediction for Y based on the i-th value of X. Then $e_i = y_i − \hat{y_i}$ represents the i-th residual: this is the difference between the i-th observed response value and the i-th response value that is predicted by our linear model.

We define the residual sum of squares (RSS) as $RSS = e^2_1 + e^2_2 + ... + e^2_n $ or equivalently as $RSS = (y_1 - \hat{\beta_0} − \hat{\beta_1} x_1 )^2 + (y_2 - \hat{\beta_0} − \hat{\beta_1} x_2 )^2 + ... + (y_n - \hat{\beta_0} − \hat{\beta_1} x_n )^2$. The least squares approach chooses $\hat{\beta_0}$ and $\hat{\beta_1}$ to minimize the RSS

In a linear model, $\beta_0$ is the expected value of Y when X = 0, and $\beta_1$ is the average increase in Y associated with a one-unit increase in X.

```{r}

# Get the squared residuals
# resid is a generic function which extracts model residuals from objects returned by modeling functions.
residuals <- resid(lm.fit)
head(residuals)

#calculate residual sum of squares (method 1)
deviance(lm.fit )

#calculate residual sum of squares (method 2)
sum(residuals^2)

#These information can be found in the summary of the linear model, along with the estimated coefficients
summary(lm.fit)

plot(dati$TV, dati$sales)
abline(lm.fit, col = "red", lwd = 2)
```

## Model Accuracy
In the reality, since the true function that models the relationship between X and Y is generally unknown, we will always have some error $\epsilon$ in our estimates. In the context of the simple linear regression model, $\epsilon$ is assumed to be normally distributed with a mean of zero.

### Residual Standard Error (RSE)
The RSE is an estimate of the standard deviation of $\epsilon$. Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is calculated as: $RSE = \sqrt{\frac{1}{(n-2)}RSS} = \sqrt{\frac{1}{(n-2)}\sum_{i=1}^{n}(y_i - \hat{y_i})^2}$.

In the case of advertising data, the RSE is 3.26. This means that the actual sales in each market deviate from the true regression line by approximately 3,260 units, on average. Another way to think about this is that even if the model were correct and the true values of the coefficients were known, any prediction of sales would still be off by about 3,260 units, on average.

The RSE is considered a measure of the lack of fit of the model to the data. If the predictions obtained by fitting the model to the training data are close to the true outcome values, then the RSE will be small, and we can conclude that the model fits the data well. If the predictions are not close to the true outcome values, then the RSE will be large, and we can conclude that the model does not fit the data well.

### R-squared
The RSE is measured in the units of Y, and it is therefore not always clear what constitutes a good RSE. The R-squared statistic provides an alternative measure of fit. It takes the form of a proportion and so it always takes on a value between 0 and 1, and is independent of the scale of Y.

To calculate the R-squared statistic, we use the formula: $R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}$, where TSS is the total sum of squares, which measures the total variance in the response Y, and is defined as: $TSS = \sum_{i=1}^{n}(y_i - \bar{y})^2$. The TSS measures the total variance in the response Y, and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, the RSS measures the amount of variability that is left unexplained after performing the regression. Hence, TSS − RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and $R^2$ measures the proportion of variability in Y that can be explained using X. An $R^2$ statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. An $R^2$ statistic that is close to 0 indicates that the regression did not explain much of the variability in the response and this might occur because the linear model is wrong, or because the inherent error $\epsilon$ is high.

### MSE (Mean Squared Error)
There is no free lunch in statistics: no one method dominates all others over all possible data sets. On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.

In order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE).
$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$

The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.

The MSE is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE. But in general, we do not really care how well the method works training on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data (test MSE).

There is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger. If the test MSE is much larger than the training MSE, then we are probably overfitting the data.

It is clear that as the level of model flexibility increases, the curves fit the training data more closely so the training MSE declines monotonically as flexibility increases. As the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.

In practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably more difficult because usually no test data are available. One important method is cross-validation, which is a method for estimating the test MSE using the training data.

### The Bias-Variance Trade-Off

The U-shape observed in the test MSE curves turns out to be the result of two competing properties of statistical learning methods. Though the mathematical proof, it is possible to show that the expected test MSE, for a given value $x_0$ , can always be decomposed into the sum of three fundamental quantities: the variance of $\hat{f}(x_0)$, the squared bias of $\hat{f}(x_0)$ and the variance of the error terms $\epsilon$.

In order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias.

Variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\hat{f}$. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in $\hat{f}$. In general, more flexible statistical methods have higher variance.

Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between $Y$ and $X_1, X_2 , . . , X_p$. It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of $f$. If $f$ is substantially non-linear, no matter how many training observations we are given, it will not be possible to produce an accurate estimate using linear regression.

As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.

Good test set performance of a statistical learning method requires low variance as well as low squared bias. This is referred to as a trade-off because it is easy to obtain a method with extremely low bias but high variance (for instance, by drawing a curve that passes through every single training observation) or a method with very low variance but high bias (by fitting a horizontal line to the data). The challenge lies in finding a method for which both the variance and the squared bias are low. This trade-off is one of the most important recurring themes in the field of statistical learning.

In a real-life situation in which f is unobserved, it is generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method. Nevertheless, one should always keep the bias-variance trade-off in mind.

# Multiple Linear Regression
The simple linear regression model assumes that there is a linear relationship between the response and the predictor. However, this is not always the case. In the advertising data, we have three predictors: TV, radio, and newspaper. We can extend the simple linear regression model to include multiple predictors. This is known as multiple linear regression.

Given p predictors, the multiple linear regression model takes the form: $Y ≈ \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$. Each $X_j$ represents a different predictor, and each $\beta_j$ quantifies the association between that predictor and the response. The $\beta_0$ term is the intercept, and is the expected value of Y when all of the predictors are equal to zero. We can interpret $\beta_j$ as the average effect on Y of a one unit increase in $X_j$, holding all other predictors fixed.

The coefficients $\beta_0, \beta_1, \beta_2, ..., \beta_p$ are unknown, and must be estimated based on the training data. We estimate these coefficients using the least squares approach as before.
We choose $\hat{\beta_0}, \hat{\beta_1}, \hat{\beta_2}, ..., \hat{\beta_p}$ to minimize the residual sum of squares (RSS), given by: $RSS = \sum_{i=1}^{n}(y_i - \hat{y_i})^2 = \sum_{i=1}^{n}(y_i - \hat{\beta_0} - \hat{\beta_1}x_{i1} - \hat{\beta_2}x_{i2} - ... - \hat{\beta_p}x_{ip})^2$.


```{r}
lm.fit <- lm(sales ~ TV + radio + newspaper, data = dati)
summary(lm.fit)

```

