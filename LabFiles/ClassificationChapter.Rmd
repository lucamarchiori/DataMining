---
title: "ClassificationChapter"
author: "Luca Marchiori"
date: "2024-04-11"
output: html_document
---

```{r setup, include=FALSE}
library(ISLR)
```

```{r}
dati = Default
summary(dati)
str(dati)
```

## Visualization of data in charts
```{r}
plot(dati$balance, dati$income, type='p', col=ifelse(dati$default == "Yes", "darkred", "lightblue"), xlab = "CC Balance", ylab = "Income")
boxplot(dati$balance ~ dati$default, xlab="Default", ylab = "CC Balance")
boxplot(dati$income ~ dati$default, xlab = "Default", ylab = "Income")
```

## Linear model
There are at least two reasons not to perform classification using a regression method: 
- (a) a regression method cannot accommodate a qualitative response with more than two classes; 
- (b) a regression method will not provide meaningful estimates of Pr(Y |X), even with just two classes. Thus, it is preferable to use a classification method that is truly suited for qualitative response values.

If we use this approach to predict default=Yes using balance, then we obtain a linear model. Here we see the problem with this approach: for balances close to zero we predict a negative probability of default, instead, if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of default, regardless of credit card balance, must fall between 0 and 1. 

This problem is not unique to the credit default data. Any time a straight line is fit to a binary response that is coded as 0 or 1, in principle we can always predict p(X) < 0 for some values of X and p(X) > 1 for others (unless the range of X is limited). To avoid this problem, we must model p(X) using a function that gives outputs between 0 and 1 for all values of X. Many functions meet this description. In logistic regression, we use the logistic function.

```{r}
# Create numeric version of factors in a new data set used for training
trainData = data.frame(studentN = as.numeric(dati$student), defaultN = as.numeric(dati$default), balance = dati$balance, income = dati$income)

# Fit the linear model
fit = lm(trainData$defaultN ~ income + balance,data = trainData)

# Test the linear model with an input similar to one in the dataset
input <- data.frame(income = 24780, studentN = 0, balance = 500)
predictedDefault <- predict(fit, newdata = input, type = "response")
predictedDefault #It predicts default = 0.985

# Test the linear model with an input strongly set to have default = 2 = YES (very low income and very high balance)
input <- data.frame(income = 0, studentN = 0, balance = 10000)
predictedDefault <- predict(fit, newdata = input, type = "response")
predictedDefault #It predicts default = 2.22 that is more than default=2 ()

plot(trainData$balance, trainData$defaultN, xlab="Default", ylab = "CC Balance")
abline(fit$coefficients["(Intercept)"], fit$coefficients["balance"])
```

## Logistic Regression
Logistic regression models the probability of default (between 0 and 1). Using a threshold we can decide if the probability is high enough to have a default = "Yes" or "No".

$$
Pr(default = Yes | balance)
$$
For low balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of X, we will obtain a sensible prediction.
$$
p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + {\beta_0 + \beta_1 X}}
$$
$$
odds = \frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X}
$$
The odds can take any value between 0 and inf. Values of the odds close to 0 and inf indicate very low and very high probabilities of default, respectively.

Example 1:
1 in 5 people will default: 1/5 = 0.2
The odd is $\frac{0.2}{1-0.2} = 1/4 = 0.25 $

Example 2:
9 in 10 people will default: 9/10 = 0.9
The odd is $\frac{0.9}{1-0.9} = 9 $

```{r}
library("aod")

trainData = NULL
trainData = data.frame(student = dati$student, default = dati$default, balance = dati$balance, income = dati$income)
# Fitting a logistic regression model
fit <- glm(default ~ student + balance + income, data = trainData, family = "binomial")
summary(fit)


# Plot the logistic regression model for each value of balance
balance_seq <- seq(min(trainData$balance), max(trainData$balance), length.out = 100) # Generate sequence of values for balance
# Create data frame with balanced values and constant income and studentN
plot_data <- expand.grid(income = mean(trainData$income), student = trainData$student, balance = balance_seq)
# Predict probabilities using the logistic regression model
plot_data$predicted_prob <- predict(fit, newdata = plot_data, type = "response")

# Plot the logistic model
library(ggplot2)
ggplot(data = trainData, aes(x = balance, y = as.numeric(default)-1)) +
  geom_point() +
  geom_line(data = plot_data, aes(y = predicted_prob), color = "blue") +
  labs(x = "Balance", y = "Default Probability") +
  ggtitle("Logistic Regression Model")


# Using the example data from the linear model above, try to predict the default
input <- data.frame(income = 24780, student = "No", balance = 500)
predictedDefault <- predict(fit, newdata = input, type = "response")
predictedDefault #It predicts default = 0.0003612339

# Test the linear model with an input strongly set to have default = 2 = YES (very low income and very high balance)
input <- data.frame(income = 0, student = "No", balance = 10000)
predictedDefault <- predict(fit, newdata = input, type = "response")
predictedDefault #It predicts default = 1

# Correctly, the predicted values are between 0 and 1

```



