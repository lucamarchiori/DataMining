---
title: "WineQualityChallenge"
author: "Luca Marchiori"
date: "2024-03-27"
output:
  html_document: default
  pdf_document: default
---
# Wine Quality Challenge
## Instructions
The inputs include objective tests (e.g. PH values) and the output is based on sensory data (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality between 0 (very bad) and 10 (very excellent).

Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the predicted value and the observed quality. 

RMSE = sqrt(mean((y â€“ haty)^2)) 

## Dataset colums
1. fixed acidity
2. volatile acidity
3. citric acid
4. residual sugar
5. chlorides
6. free sulfur dioxide
7. total sulfur dioxide
8. density
9. pH
10. sulphates
11. alcohol
12. quality (score between 0 and 10)
## Data import
```{r}
# Load the training data
train <- read.csv("wineq_train.csv", stringsAsFactors=F)
# Load the test data
test <- read.csv("wineq_validation.csv", , stringsAsFactors=F)
```

## Exploratory Data Analysis
```{r}
# Compactly Display the Structure of an Arbitrary R Object
str(train)

# Take an initial view of the dataset
summary(train)
```
### Fit a basic linear model
The lm function is used to create linear models.
`quality ~ .` is the formula provided to the lm() function. In R modeling formulas, the tilde ~ separates the outcome variable from the predictor variables. In this case, quality is the outcome variable, and . means "all other variables in the data frame". So essentially, it's saying "predict quality using all other variables in the train data frame".

`data=train` specifies the data frame to be used for fitting the model. In this case, it's using the train data frame.

`predict()` is a function in R used to generate predictions from various types of models, including linear regression models. The argument `newdata=test` specifies the data frame (test) for which we want to predict the outcome quality variable.

In summary, use the linear regression model (`fit`) to predict the outcome variable for the observations in the test data frame, and store these predictions in the variable `yhat`.

```{r}
fit = lm(quality ~ ., data=train)
summary(fit)
yhat = predict(fit, newdata=test)

# Put the prediction in a file for the submission on the platform
write.table(file="mySubmission.txt", yhat, row.names = FALSE, col.names = FALSE)
```

## Computing Minimal Squared Error
MSE (Mean Squared Error) is a common metric for evaluating the performance of regression models, by comparing the observed values `y` with the predicted values `yhat` generated by the provided `model`.
```{r}
MSE <- function(y, model){
  yhat = predict(model) # Estimated y computed on the basis of the features
  mean((y-yhat)^2)
}

SQRTMSE <- function(y, model){
  sqrt(MSE(y,model))
}
```

## Get MSE for basic linear model
```{r}
mse = MSE(y=train$quality, model=fit)
# Square root of MSE
sqrt(mse)
```

```{r}
#library(ggplot2)
#require(graphics)
#pairs(train)
#ggplot(cor(train))

#ggcorrplot(cor(train))
#heatmap(cor(train))

#plot(train$alcohol, train$quality)
#plot(train$total.sulfur.dioxide, train$free.sulfur.dioxide)
#abline(fit)
#cor(train)
```

## Optimizing linear model
Create a new linear model without considering citric.acid, chlorides and total.sulfur.dioxide
We get a SQMSE of 0.7534613 which is good
```{r}
fit_m3 = lm(quality ~ . -citric.acid -chlorides -total.sulfur.dioxide, data=train)
yhat = predict(fit_m3, newdata=test)
SQRTMSE(y=train$quality, model=fit_m3)
write.table(file="mySubmission.txt", yhat, row.names = FALSE, col.names = FALSE)
```

Taking out also density, leads to a SQRTMSE of 0.7583856 which is worse
```{r}
fit_m3n2 = lm(quality ~ . -citric.acid - chlorides - total.sulfur.dioxide - density, data=train) 
yhat = predict(fit_m3n2, newdata=test)
SQRTMSE(y=train$quality, model=fit_m3n2)
```

Taking out residual sugar but not density, leads to a SQRTMSE of 0.7627304 which is worse
```{r}
fit_m3n3 = lm(quality ~ . -citric.acid - chlorides - total.sulfur.dioxide - residual.sugar, data=train) 
yhat = predict(fit_m3n3, newdata=test)
SQRTMSE(y=train$quality, model=fit_m3n3)
```


Find the best SQRTMSE by trying all the possible combinations of features
```{r}
# Create all possible combinations of features
all_features <- colnames(train)  # Assuming 'train' is your training data frame
all_features <- setdiff(all_features, "quality")  # Remove the dependent variable
feature_combinations <- lapply(1:length(all_features), function(x) combn(all_features, x))

# Fit models and calculate RMSE for each combination
best_rmse <- Inf
best_model <- NULL
for (i in seq_along(feature_combinations)) {
  combinations <- feature_combinations[[i]]
  for (j in 1:ncol(combinations)) {
    formula <- as.formula(paste("quality ~", paste(combinations[, j], collapse = " + ")))
    model <- lm(formula, data = train)
    rmse <- SQRTMSE(train$quality, model)
    if (rmse < best_rmse) {
      best_rmse <- rmse
      best_model <- model
    }
  }
}

summary(best_model)
SQRTMSE(y=train$quality, model=best_model)
yhat = predict(best_model, newdata=test)
write.table(file="mySubmission.txt", yhat, row.names = FALSE, col.names = FALSE)
```


