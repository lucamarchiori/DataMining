---
title: "BeyondLinearityChapter"
author: "Luca Marchiori"
date: "2024-05-11"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
# Import libraries
library(ISLR)
library(ISLR2)
library(boot)
library(mgcv)
# Clear workspace
rm(list = ls())

# Set seed
set.seed(1)

# Load data
ds = Wage
```

## Polynomial Regression
Polynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. This approach provides a simple way to build a nonlinear fit to data. For large enough degree d, a polynomial regression allows us to produce an extremely non-linear curve. Generally speaking, it is unusual to use d greater than 3 or 4 because for large values of d, the polynomial curve can become overly flexible and can take on some very strange shapes.

$$
Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + ... + \beta_dX^d + \epsilon
$$
Notice that the coefficients can be easily estimated using least squares linear regression because this is just a standard linear model with predictors $X, X^2, X^3, ..., X^d$.

```{r}
# Fit a linear model, using the lm() function, in order to predict wage using a fourth-degree polynomial in age
fit <- lm(wage ~ poly(age , 4), data = ds)
summary(fit)

# Create a grid of values for age at which we want predictions
age.grid <- seq(0, 100)

# Predict wage for all ages
preds <- predict(fit , newdata = list(age = age.grid), se = TRUE)

# Plot age and wage from the dataset
plot(ds$age , ds$wage , cex = .5, col = "darkgrey")

# Predict wage for all ages
lines(age.grid , preds$fit , lwd = 2, col = "blue")
```


## Step Functions
By using Step Functions, we break the range of X into bins, and fit a different constant in each bin. 
Unless there are natural breakpoints in the predictors, piece wise constant functions can miss the action. Step function approaches are very popular in bio statistics and epidemiology, among other disciplines. For example, 5-year age groups are often used to define the bins.

We create cutpoints $c_1, c_2, ..., c_K$ in the range of X, and then construct $K + 1$ new variables.
$$
C_0(X) = I(X < c_1) \\
C_1(X) = I(c_1 \leq X < c_2) \\
C_2(X) = I(c_2 \leq X < c_3) \\
... \\
$$
Note that $I()$ is the indicator function that returns a 1 if the condition is true, and 0 otherwise (these are sometimes called dummy variables).  For a given value of X, at most one of these $K + 1$ variables will be non-zero.

It is then possible to use least squares to fit a linear model using $C_0(X), C_1(X), ..., C_K(X)$ as predictors:
$$
Y = \beta_0 + \beta_1C_1(x_i) + \beta_2C_2(x_i) + ... + \beta_KC_K(x_i) + \epsilon
$$

## Basis Functions
Polynomial and piecewise-constant regression models are in fact special cases of a basis function approach.
$$
y_i = \beta_0 + \beta_1b_1(x_i) + \beta_2b_2(x_i) + ... + \beta_Kb_K(x_i) + \epsilon
$$
where $b_1(), b_2(), ..., b_K()$ are a set of functions typically fixed and known, and are not estimated from the data.

## Regression Splines
Piecewise polynomial regression involves fitting separate low-degree polynomials over different regions of $X$. The points where the polynomial pieces meet are called knots.

For example, a piecewise cubic polynomial fitting a cubic regression model with a single knot c is of the form:
$$
y_i = \left\{\begin{matrix}
 & \beta_{01} + \beta_{11}x_i + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon & if \ x_i < c \\
 & \beta_{02} + \beta_{12}x_i + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon & if \ x_i \ge c
\end{matrix}\right.
$$
where $\beta_0, \beta_1, \beta_2, \beta_3$ are different in different parts of the range of $X$ (before and after the knot c). Using more knots leads to a more flexible piecewise polynomial. Each polynomial function can be fit using least squares.

To avoid having discontinuities in the first and second derivatives at the knot locations, we can fit a piecewise polynomial under the constraint that the fitted curve must be continuous at each knot.

To add smoothness to the model at the knot locations, we can require that not only the fitted curve is continuous at each knot, but also that its first and second derivatives are continuous. This regression splines can be somewhat complex since we must fit a piecewise degree-d polynomial under the constraint that it is continuous at all points, and that its first and second derivatives are also continuous. To do this, we can use basis functions. 

A cubic spline with K knots can be modeled as:

$$
y_i = \beta_0 + \beta_1b_1(x_i) + \beta_2b_2(x_i) + ... + \beta_Kb_K(x_i) + \epsilon
$$
where $b_1(), b_2(), ..., b_K()$ are basis functions and K are the number of knots. The most direct way to represent a cubic spline is to start off with a basis for a cubic polynomial (i.e. $x, x^2, x^3$) and then add one truncated power basis function per knot.

The regression spline is created by specifying a set of knots, producing a set of basis functions and then using least squares to estimate the spline coefficients.

Unfortunately, splines can have high variance at the outer range of the predictors. A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where $X$ is smaller than the smallest knot, and in the region where $X$ is larger than the largest knot). This constraint makes the function more stable, and hence reduces the variance, at the boundaries.

The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. While this option can work well, in practice it is common to place knots in a uniform fashion. One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data

A good way to choose the number of knots (degrees of freedom) is to use cross-validation. We repeat cross validation for different numbers of knots, and choose the number of knots that minimizes the RSS.

Regression splines often give superior results to polynomial regression. This is because unlike polynomials, which must use a high degree to produce flexible fits, splines introduce flexibility by increasing the number of knots but keeping the degree fixed. Generally, this approach produces more stable estimates. Splines also allow us to place more knots, and hence flexibility, over regions where the function f seems to be changing rapidly, and fewer knots where f appears more stable.

## Smoothing Splines
What we are looking for, is to find a functions $g(x)$ that fits the observed data well by having $$ RSS = \sum_{i=1}^{n} (y_i - g(x_i))^2 $$ as small as possible. The problem with this is that if we don't put any constraints on $g(x)$, it will pass through every single point in the dataset, making $$RSS = 0$$, and leading to overfitting. We instead want a function that is smooth.

A natural approach to add smoothness is to find the function $g(x)$ that minimizes:
$$
\sum_{i=1}^{n} (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt
$$
where $\lambda$ is a non-negative tuning parameter that controls the amount of smoothness. The function $g(x)$ that minimizes this equation is called a smoothing spline.

The larger $\lambda$ is, the smoother the function $g(x)$ will be because $\lambda$ controls the impact of the penalty term.
The penalty term $$ \int g''(t)^2 dt $$ is the integral of the squared second derivative of $g(x)$. The first derivative of a function is its slope, and the second derivative is the rate of change of the slope.
The integral notation is used to represent the total sum of the variability of $g(x)$, and hence the total smoothness of the function. The larger the value of the integral, the less smooth the function $g(x)$ will be.

When $\lambda = 0$, the penalty term has no effect, and the smoothing spline will interpolate the data. When $\lambda$ is very large, the penalty term will dominate the criterion, and the function $g(x)$ will be very smooth. The tuning parameter $\lambda$ controls the trade-off between the fit to the training data and the smoothness of the function $g(x)$ and hence the effective degrees of freedom of the model.

The function $g(x)$ is a piecewise cubic polynomial with knots at each unique values of $$ x_1, x_2, ..., x_n $$, and with a continuous first and second derivative. The function $g(x)$ is also linear in the regions outside the range of the $x_i$ (knots).

In fitting a smoothing spline, we do not need to select the number of knots, as the smoothing spline will automatically place a knot at each training observation. Instead, we need to select the best value of the tuning parameter $\lambda$ such that, the RSS is minimized in cross-validation. An efficient way to do this is LOOCV.

## Generalized Additive Models
Until now we have explored approaches for fitting a non-linear function to a single predictor. However, in many cases we have more than one predictor, and we would like to extend the non-linear regression models to accommodate multiple predictors. One way to do this is to use a generalized additive model (GAM).

### GAMs for Regression

Multiple linear regression model (chapter 3):
$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon
$$
Multiple non linear regression model (additive model):
$$
y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + ... + f_p(x_{ip}) + \epsilon = \beta_0 + \sum_{j=1}^{p} f_j(x_{ij}) + \epsilon
$$
Each non-linear function for each predictor is calculated separately and then added togheter.

GAMs allow us to fit a non-linear $$f_i$$ to each $$X_j$$, so that we can automatically model non-linear relationships that standard linear regression will miss. This means that we do not need to manually try out many different transformations on each variable individually.

The non-linear fits can potentially make more accurate predictions for the response $$Y$$.
Because the model is additive, we can examine the effect of each $$X_j$$ on $$y$$ individually while holding all of the other variables fixed.
The smoothness of the function $$f_j$$ for the variable $$X_j$$ can be summarized via degrees of freedom.

```{r}
library(caret)

ds$education <- gsub('1. < HS Grad', 'ltHS', ds$education)
ds$education <- gsub('2. HS Grad', 'HS', ds$education)
ds$education <- gsub('3. Some College', 'SomeCollege', ds$education)
ds$education <- gsub('4. College Grad', 'College', ds$education)
ds$education <- gsub('5. Advanced Degree', 'AdvancedDegree', ds$education)

ds$race <- gsub('1. White', 'White', ds$race)
ds$race <- gsub('2. Black', 'Black', ds$race)
ds$race <- gsub('3. Asian', 'Asian', ds$race)
ds$race <- gsub('4. Other', 'Other', ds$race)

ds$health_ins <- gsub('1. Yes', 'Yes', ds$health_ins)
ds$health_ins <- gsub('2. No', 'No', ds$health_ins)

ds$health <- gsub('1. <=Good', 'ltGood', ds$health)
ds$health <- gsub('2. >=Very Good', 'gtVeryGood', ds$health)


caretGam <- caret::train(wage ~ age + year + education + race + health_ins + health,
                         data = ds, 
                         method = "bam",
                         preProcess = c('center', 'scale'), 
                         family = "gaussian",
                         trControl = trainControl(method = "CV", number=2))
                         
```


```{r}
gamModel <- caretGam$finalModel
summary(gamModel)
gamModel$formula
vis.gam(gamModel, view = c("age", "year"))
plot.gam(gamModel, shade = TRUE)
```





























